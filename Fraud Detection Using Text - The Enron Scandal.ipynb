{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763cb0ff",
   "metadata": {},
   "source": [
    "# Fraud Detection Using Text - The Enron Scandal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224bc4ef",
   "metadata": {},
   "source": [
    "![enron](enron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ff22d",
   "metadata": {},
   "source": [
    "## Welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee6fc7",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408dced",
   "metadata": {},
   "source": [
    "## Important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2a015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas & numpy:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import plotly.express as px\n",
    "\n",
    "# tokenization:\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "# gensim:\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# scikit-learn:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#keras:\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #We use this because some text are shorter than others. All text should have the same number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3852bf",
   "metadata": {},
   "source": [
    "## Loading & understanding our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e05c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData/emails_cleaned.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# With \"pd.set_option('display.max_columns', None)\" we see all the columns of the dataset.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1253\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1251\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1253\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1147\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Da_Env1\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1429\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1425\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1426\u001b[0m     )\n\u001b[1;32m-> 1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('Data/emails_cleaned.csv')\n",
    "# With \"pd.set_option('display.max_columns', None)\" we see all the columns of the dataset.\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5974bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "    df=df.drop(columns=['file', 'message', 'Cc', 'Mime-Version', 'Content-Type','Content-Transfer-Encoding','Date',\n",
    "                        'Bcc', 'X-From', 'X-To', 'X-cc', 'X-bcc','X-Folder', 'X-Origin', 'X-FileName', 'has_other_content', 'if_forwarded'])\n",
    "    df.columns=[e.lower().replace(' ', '_') for e in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a19c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=clean_data(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna().reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402027bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to join all text data in one single column.We will create a new column called \"completed_text\"\n",
    "df[\"completed_text\"]=df[\"subject\"]+df[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d172d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c123141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how looks the first element of the column \"completed_text\"\n",
    "df['completed_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ad9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have joined the columns \"subject\" and \"content\" into one column, we can delete them \n",
    "df=df.drop(columns=['content', 'subject'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fccd11",
   "metadata": {},
   "source": [
    "## Fraud flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19124e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83043060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have all the text data together, we can flag some terms as \"fraud suspect\"\n",
    "# WE know that Enron employees activaley participated in the fraud by keeping the stock price manually high. \n",
    "# We can create a list of fraudulent terms that helps us to find the emails with reference to the word \"stock\".\n",
    "\n",
    "fraud_list=['stock','enron stock','sell stock','bonus','wall street','board','the market','dow jones']\n",
    "\n",
    "# Now we filter the column completed text using the list \"fraud_list\".\n",
    "\n",
    "\n",
    "filtered_emails = df.loc[df['completed_text'].str.contains('|'.join(fraud_list), na=False)]\n",
    "print(filtered_emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc788c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Filtered_emails\" represents the emails with fraudulent terminology. \n",
    "# We will create a new column named fraud in the dataframe. \n",
    "# The new column will have 2 values: 0: non-fraud ; 1: fraud\n",
    "\n",
    "df['fraud'] = np.where((df['completed_text'].str.contains('|'.join(fraud_list)) == True), 1, 0)\n",
    "        \n",
    "df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['fraud'].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a10953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the count plot for fraudulent and no fraudulent emails\n",
    "plt.figure(figsize = (4, 4))\n",
    "sns.countplot(y = \"fraud\", data = df,palette='vlag')\n",
    "plt.title('Fraud Distributions \\n (0:Non_Fraud || 1: Fraud)', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f4c73",
   "metadata": {},
   "source": [
    "## Detecting fraud using text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbee7a",
   "metadata": {},
   "source": [
    "In order to clean our text data we need to follow these steps:\n",
    "    <ol>\n",
    "  <li>Tokenization</li>\n",
    "  <li>Remove all Stopwords</li>\n",
    "  <li>Lemanize your words</li>\n",
    "  <li>Stem your words</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now he are going to get the stopwords for English\n",
    "stop_words = list(stopwords.words('english')) \n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46087427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get additional stop words from nltk\n",
    "stop_words.extend(['from','to','cc','http', 're', 'www', 'com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and remove words with 2 or less characters\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a36181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataframe\n",
    "df['clean_text'] = df['completed_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compare the columns \"completed_text\" and \"clean_text\"\n",
    "#Show completed_text news\n",
    "df['completed_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04016329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['clean_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the total words present in the dataset\n",
    "list_of_words = []\n",
    "for i in df.clean_text:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed232420",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3385f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the total number of unique words\n",
    "total_words = len(list(set(list_of_words)))\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the words into a string\n",
    "df['clean_joined'] = df['clean_text'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76348a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_joined'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884b0b39",
   "metadata": {},
   "source": [
    "## Visualize cleaned up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the word cloud for text that is considered \"fraud\"\n",
    "plt.figure(figsize = (7,7)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1920 , height = 1080 ,colormap='vlag', stopwords = stop_words).generate(\" \".join(df[df.fraud == 1].clean_joined))\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e387ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the word cloud for text that is considered \"non-fraud\"\n",
    "plt.figure(figsize = (7,7)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1920 , height = 1080 ,colormap='vlag', stopwords = stop_words).generate(\" \".join(df[df.fraud == 0].clean_joined))\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f882cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of maximum document will be needed to create word embeddings \n",
    "maxlen = -1\n",
    "for doc in df.clean_joined:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any email is =\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of number of words in an email.\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "sns.histplot(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], color='#366f88', bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a1d16",
   "metadata": {},
   "source": [
    "## Prepare the data by perfoming tokenization and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85b083",
   "metadata": {},
   "source": [
    "Tokenizer allows us to vectorize text corpus by turning each text into a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfbcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['clean_joined']\n",
    "y=df['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer to tokenize the words and create sequences of tokenized words\n",
    "tokenizer = Tokenizer(num_words = total_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The encoding for document:\\n\",df.clean_joined[0],\"\\n is : \",train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28554192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding can either be maxlen = 4406 or smaller number maxlen = 40 seems to work well based on results\n",
    "padded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')\n",
    "padded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,doc in enumerate(padded_train[:1]):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4092b3",
   "metadata": {},
   "source": [
    "## Built the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= RandomForestClassifier(random_state=42, n_estimators=10, max_depth=None)\n",
    "\n",
    "# embeddidng layer\n",
    "\n",
    "\n",
    "result=model.fit(X_train, y_train)\n",
    "y_pred=result.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Classification report for the test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30ca36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
